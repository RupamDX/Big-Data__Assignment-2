{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE THIS TO CONVERT JSON TO CSV TABLES THEN JSON TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if 'ticker' column exists in sub.json...\n",
      "SUB Data Columns BEFORE Fix: Index(['adsh', 'cik', 'name', 'sic', 'countryba', 'stprba', 'cityba', 'zipba',\n",
      "       'bas1', 'bas2', 'baph', 'countryma', 'stprma', 'cityma', 'zipma',\n",
      "       'mas1', 'mas2', 'countryinc', 'stprinc', 'ein', 'former', 'changed',\n",
      "       'afs', 'wksi', 'fye', 'form', 'period', 'fy', 'fp', 'filed', 'accepted',\n",
      "       'prevrpt', 'detail', 'instance', 'nciks', 'aciks', 'ticker'],\n",
      "      dtype='object')\n",
      "SUB Data Sample AFTER Fix:\n",
      "                    adsh      cik                     name   sic countryba  \\\n",
      "0  0001047469-09-009543    24545  MOLSON COORS BREWING CO  2082        US   \n",
      "1  0001047469-09-009754   831001            CITIGROUP INC  6021        US   \n",
      "2  0001157523-09-007839  1067983   BERKSHIRE HATHAWAY INC  6331        US   \n",
      "3  0000918160-09-000039   918160    AK STEEL HOLDING CORP  3312        US   \n",
      "4  0001144204-09-056412  1121788               GARMIN LTD  3812        KY   \n",
      "\n",
      "  stprba        cityba     zipba                                 bas1  \\\n",
      "0     CO        GOLDEN     80401           P.O. BOX 4030, MAIL #NH375   \n",
      "1     NY      NEW YORK     10043                      399 PARK AVENUE   \n",
      "2     NE         OMAHA     68131                      1440 KIEWIT PLZ   \n",
      "3     OH  WEST CHESTER     45069             9227 CENTRE POINTE DRIVE   \n",
      "4           CAMANA BAY  KY1-1006  PO BOX 10670, GRAND CAYMAN KY1-1006   \n",
      "\n",
      "                                      bas2  ...    fy  fp     filed  \\\n",
      "0                                           ...  2009  Q3  20091105   \n",
      "1                                           ...  2009  Q3  20091106   \n",
      "2                                           ...  2009  Q3  20091106   \n",
      "3                                           ...  2009  Q3  20091103   \n",
      "4  STE. 3206B, 45 MARKET ST., GARDENIA CT.  ...  2009  Q3  20091104   \n",
      "\n",
      "                accepted prevrpt detail           instance nciks aciks  \\\n",
      "0  2009-11-04 21:16:00.0       0      0   tap-20090926.xml     1         \n",
      "1  2009-11-06 17:12:00.0       0      0     c-20090930.xml     1         \n",
      "2  2009-11-06 17:05:00.0       0      0  brka-20090930.xml     1         \n",
      "3  2009-11-03 09:06:00.0       0      0   aks-20090930.xml     1         \n",
      "4  2009-11-04 11:11:00.0       0      0  grmn-20090926.xml     1         \n",
      "\n",
      "    ticker  \n",
      "0  UNKNOWN  \n",
      "1  UNKNOWN  \n",
      "2  UNKNOWN  \n",
      "3  UNKNOWN  \n",
      "4  UNKNOWN  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "Final SUB Columns: Index(['adsh', 'cik', 'filed', 'fy', 'fp', 'ticker'], dtype='object')\n",
      "Sample SUB Data:\n",
      "                    adsh      cik      filed    fy  fp   ticker\n",
      "0  0001047469-09-009543    24545 2009-11-05  2009  Q3  UNKNOWN\n",
      "1  0001047469-09-009754   831001 2009-11-06  2009  Q3  UNKNOWN\n",
      "2  0001157523-09-007839  1067983 2009-11-06  2009  Q3  UNKNOWN\n",
      "3  0000918160-09-000039   918160 2009-11-03  2009  Q3  UNKNOWN\n",
      "4  0001144204-09-056412  1121788 2009-11-04  2009  Q3  UNKNOWN\n",
      "NUM Sample:\n",
      "                    adsh                                                tag  \\\n",
      "0  0000040545-09-000074     EarningsFromContinuingOperationsRetainedByGECS   \n",
      "1  0001398432-09-000470                                         ProfitLoss   \n",
      "2  0001193125-09-205617  StockholdersEquityIncludingPortionAttributable...   \n",
      "3  0000950123-09-057759                        DividendsPreferredStockCash   \n",
      "4  0000950123-09-059334               CommonStockDividendsPerShareDeclared   \n",
      "\n",
      "                version      ddate qtrs   uom  \\\n",
      "0  0000040545-09-000074 2008-09-30    3   USD   \n",
      "1          us-gaap/2009 2008-09-30    3   USD   \n",
      "2          us-gaap/2009 2008-12-31    0   USD   \n",
      "3          us-gaap/2009 2009-09-30    3   USD   \n",
      "4          us-gaap/2009 2008-09-30    3  pure   \n",
      "\n",
      "                                            segments             coreg  \\\n",
      "0                      LegalEntity=SubsidiariesGECS;  SubsidiariesGECS   \n",
      "1           EquityComponents=NoncontrollingInterest;                     \n",
      "2                 EquityComponents=RetainedEarnings;                     \n",
      "3  ClassOfStock=SeriesBPreferredStock;EquityCompo...                     \n",
      "4                 EquityComponents=RetainedEarnings;                     \n",
      "\n",
      "          value footnote  \n",
      "0  0.000000e+00           \n",
      "1  2.761800e+07           \n",
      "2  3.063800e+10           \n",
      "3 -5.742000e+06           \n",
      "4  5.625000e-01           \n",
      "PRE Sample:\n",
      "                    adsh                        tag statement_type\n",
      "0  0000004904-09-000174     ElectricUtilityRevenue             IS\n",
      "1  0000004904-09-000174       OtherSalesRevenueNet             IS\n",
      "2  0000004904-09-000174                   Revenues             IS\n",
      "3  0000004904-09-000174  ElectricProductionExpense             IS\n",
      "4  0000004904-09-000174       CostOfPurchasedPower             IS\n",
      "TAG Sample:\n",
      "                                            tag  \\\n",
      "0                AccountsAndNotesReceivableNet   \n",
      "1    AccountsNotesAndLoansReceivableNetCurrent   \n",
      "2                              AccountsPayable   \n",
      "3         AccountsPayableAndAccruedLiabilities   \n",
      "4  AccountsPayableAndAccruedLiabilitiesCurrent   \n",
      "\n",
      "                                              tlabel  \n",
      "0  Accounts and Financing Receivable, after Allow...  \n",
      "1  Accounts and Financing Receivable, after Allow...  \n",
      "2           Accounts Payable (Deprecated 2009-01-31)  \n",
      "3  Accounts Payable and Accrued Liabilities (Depr...  \n",
      "4  Accounts Payable and Accrued Liabilities, Current  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load JSON files into Pandas DataFrames\n",
    "def load_json(file_path):\n",
    "    \"\"\" Load JSON file and convert it to a Pandas DataFrame \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# File paths (update as per actual locations)\n",
    "num_file = \"D:/NEU/SEM2/BigData/ASGN2/TEST/jsons2/2009q4/num.json\"\n",
    "pre_file = \"D:/NEU/SEM2/BigData/ASGN2/TEST/jsons2/2009q4/pre.json\"\n",
    "sub_file = \"D:/NEU/SEM2/BigData/ASGN2/TEST/jsons2/2009q4/sub.json\"\n",
    "tag_file = \"D:/NEU/SEM2/BigData/ASGN2/TEST/jsons2/2009q4/tag.json\"\n",
    "\n",
    "# Load data\n",
    "num_df = load_json(num_file)\n",
    "pre_df = load_json(pre_file)\n",
    "sub_df = load_json(sub_file)\n",
    "tag_df = load_json(tag_file)\n",
    "\n",
    "# Clean NUM data\n",
    "num_df[\"value\"] = pd.to_numeric(num_df[\"value\"], errors=\"coerce\")  # Convert to float\n",
    "num_df[\"ddate\"] = pd.to_datetime(num_df[\"ddate\"], format=\"%Y%m%d\", errors=\"coerce\")  # Convert to datetime\n",
    "num_df.dropna(subset=[\"value\"], inplace=True)  # Drop rows where value is NaN\n",
    "\n",
    "# Clean PRE data\n",
    "pre_df = pre_df[[\"adsh\", \"tag\", \"stmt\"]]  # Keep relevant columns\n",
    "pre_df.rename(columns={\"stmt\": \"statement_type\"}, inplace=True)  # Rename for clarity\n",
    "\n",
    "# Debugging: Check if ticker column exists\n",
    "print(\"Checking if 'ticker' column exists in sub.json...\")\n",
    "print(\"SUB Data Columns BEFORE Fix:\", sub_df.columns)\n",
    "\n",
    "# Ensure 'ticker' exists in SUB data\n",
    "if \"ticker\" not in sub_df.columns:\n",
    "    print(\"Warning: 'ticker' column not found in sub.json! Adding it as NaN.\")\n",
    "    sub_df[\"ticker\"] = None  # Assign NaN if missing\n",
    "\n",
    "# Debugging: Show sample data\n",
    "print(\"SUB Data Sample AFTER Fix:\\n\", sub_df.head())\n",
    "\n",
    "# Clean SUB data\n",
    "required_columns = [\"adsh\", \"cik\", \"filed\", \"fy\", \"fp\"]\n",
    "if \"ticker\" in sub_df.columns:\n",
    "    required_columns.append(\"ticker\")  # Include ticker only if present\n",
    "\n",
    "sub_df = sub_df[required_columns]  # Keep only necessary columns\n",
    "sub_df[\"filed\"] = pd.to_datetime(sub_df[\"filed\"], format=\"%Y%m%d\", errors=\"coerce\")  # Convert to datetime\n",
    "\n",
    "# Clean TAG data\n",
    "tag_df = tag_df[[\"tag\", \"tlabel\"]]  # Keep only tag and label\n",
    "\n",
    "# Debugging check\n",
    "print(\"Final SUB Columns:\", sub_df.columns)\n",
    "print(\"Sample SUB Data:\\n\", sub_df.head())\n",
    "print(\"NUM Sample:\\n\", num_df.head())\n",
    "print(\"PRE Sample:\\n\", pre_df.head())\n",
    "print(\"TAG Sample:\\n\", tag_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance Sheet Sample:\n",
      "     ticker      cik      filed    fy  fp  \\\n",
      "0  UNKNOWN    77476 2009-10-08  2009  Q3   \n",
      "1  UNKNOWN   103379 2009-11-10  2009  Q3   \n",
      "2  UNKNOWN    18230 2009-10-30  2009  Q3   \n",
      "3  UNKNOWN  1326380 2009-12-09  2009  Q3   \n",
      "4  UNKNOWN   915912 2009-11-09  2009  Q3   \n",
      "\n",
      "                                                 tag  \\\n",
      "0  StockholdersEquityIncludingPortionAttributable...   \n",
      "1  StockholdersEquityIncludingPortionAttributable...   \n",
      "2  StockholdersEquityIncludingPortionAttributable...   \n",
      "3                               FixturesAndEquipment   \n",
      "4                 AccruedExpensesAndOtherLiabilities   \n",
      "\n",
      "                                              tlabel         value      ddate  \n",
      "0  Stockholders' Equity, Including Portion Attrib...  3.063800e+10 2008-12-31  \n",
      "1  Stockholders' Equity, Including Portion Attrib...  1.108140e+08 2009-09-30  \n",
      "2  Stockholders' Equity, Including Portion Attrib...  3.057000e+09 2008-12-31  \n",
      "3                             Fixtures and equipment  6.198450e+08 2009-01-31  \n",
      "4             Accrued expenses and other liabilities  2.276210e+08 2008-12-31  \n"
     ]
    }
   ],
   "source": [
    "# Filter Balance Sheet items\n",
    "bs_pre = pre_df[pre_df[\"statement_type\"] == \"BS\"]\n",
    "\n",
    "# Merge with NUM data\n",
    "balance_sheet = num_df.merge(bs_pre, on=[\"adsh\", \"tag\"], how=\"inner\")\n",
    "\n",
    "# Merge with SUB data to get filing info\n",
    "balance_sheet = balance_sheet.merge(sub_df, on=\"adsh\", how=\"inner\")\n",
    "\n",
    "# Merge with TAG data for descriptions\n",
    "balance_sheet = balance_sheet.merge(tag_df, on=\"tag\", how=\"left\")\n",
    "\n",
    "# Keep only required columns, including TICKER\n",
    "balance_sheet = balance_sheet[[\"ticker\", \"cik\", \"filed\", \"fy\", \"fp\", \"tag\", \"tlabel\", \"value\", \"ddate\"]]\n",
    "\n",
    "print(\"Balance Sheet Sample:\\n\", balance_sheet.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income Statement Sample:\n",
      "     ticker      cik      filed    fy  fp  \\\n",
      "0  UNKNOWN   879101 2009-11-05  2009  Q3   \n",
      "1  UNKNOWN    91576 2009-11-06             \n",
      "2  UNKNOWN     5272 2009-11-06  2009  Q3   \n",
      "3  UNKNOWN     5272 2009-11-06  2009  Q3   \n",
      "4  UNKNOWN  1385157 2009-11-18  2009  FY   \n",
      "\n",
      "                                                 tag  \\\n",
      "0                                         ProfitLoss   \n",
      "1               CommonStockDividendsPerShareDeclared   \n",
      "2  OtherThanTemporaryImpairmentLossesInvestmentsA...   \n",
      "3  OtherThanTemporaryImpairmentLossesInvestmentsA...   \n",
      "4     RestructuringAndAssetImpairmentAndOtherCharges   \n",
      "\n",
      "                                              tlabel         value      ddate  \n",
      "0  Net Income (Loss), Including Portion Attributa...  2.761800e+07 2008-09-30  \n",
      "1       Common Stock, Dividends, Per Share, Declared  5.625000e-01 2008-09-30  \n",
      "2  Other-than-temporary Impairment Losses, Invest...  1.970600e+10 2008-09-30  \n",
      "3  Other Than Temporary Impairment Losses, Invest...  1.970600e+10 2008-09-30  \n",
      "4  Restructuring and Asset Impairment and Other C...  3.750000e+08 2009-09-30  \n"
     ]
    }
   ],
   "source": [
    "# Filter Income Statement items\n",
    "is_pre = pre_df[pre_df[\"statement_type\"] == \"IS\"]\n",
    "\n",
    "# Merge with NUM data\n",
    "income_statement = num_df.merge(is_pre, on=[\"adsh\", \"tag\"], how=\"inner\")\n",
    "\n",
    "# Merge with SUB data to get filing info\n",
    "income_statement = income_statement.merge(sub_df, on=\"adsh\", how=\"inner\")\n",
    "\n",
    "# Merge with TAG data for descriptions\n",
    "income_statement = income_statement.merge(tag_df, on=\"tag\", how=\"left\")\n",
    "\n",
    "# Keep only required columns, including TICKER\n",
    "income_statement = income_statement[[\"ticker\", \"cik\", \"filed\", \"fy\", \"fp\", \"tag\", \"tlabel\", \"value\", \"ddate\"]]\n",
    "\n",
    "print(\"Income Statement Sample:\\n\", income_statement.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cash Flow Sample:\n",
      "     ticker      cik      filed    fy  fp  \\\n",
      "0  UNKNOWN    40545 2009-11-06  2009  Q3   \n",
      "1  UNKNOWN    40545 2009-11-06  2009  Q3   \n",
      "2  UNKNOWN   879101 2009-11-05  2009  Q3   \n",
      "3  UNKNOWN  1403161 2009-11-20  2009  FY   \n",
      "4  UNKNOWN  1403161 2009-11-20  2009  FY   \n",
      "\n",
      "                                                 tag  \\\n",
      "0     EarningsFromContinuingOperationsRetainedByGECS   \n",
      "1     EarningsFromContinuingOperationsRetainedByGECS   \n",
      "2                                         ProfitLoss   \n",
      "3  NoncashOrPartNoncashAcquisitionNonrecourseDebt...   \n",
      "4  NoncashOrPartNoncashAcquisitionNonrecourseDebt...   \n",
      "\n",
      "                                              tlabel       value      ddate  \n",
      "0  Earnings from continuing operations retained b...         0.0 2008-09-30  \n",
      "1  Earnings from continuing operations retained b...         0.0 2008-09-30  \n",
      "2  Net Income (Loss), Including Portion Attributa...  27618000.0 2008-09-30  \n",
      "3  Non-recourse debt assumed in acquisition of su...         0.0 2008-09-30  \n",
      "4  Assets acquired in joint venture with note pay...         0.0 2008-09-30  \n"
     ]
    }
   ],
   "source": [
    "# Filter Cash Flow items\n",
    "cf_pre = pre_df[pre_df[\"statement_type\"] == \"CF\"]\n",
    "\n",
    "# Merge with NUM data\n",
    "cash_flow = num_df.merge(cf_pre, on=[\"adsh\", \"tag\"], how=\"inner\")\n",
    "\n",
    "# Merge with SUB data to get filing info\n",
    "cash_flow = cash_flow.merge(sub_df, on=\"adsh\", how=\"inner\")\n",
    "\n",
    "# Merge with TAG data for descriptions\n",
    "cash_flow = cash_flow.merge(tag_df, on=\"tag\", how=\"left\")\n",
    "\n",
    "# Keep only required columns, including TICKER\n",
    "cash_flow = cash_flow[[\"ticker\", \"cik\", \"filed\", \"fy\", \"fp\", \"tag\", \"tlabel\", \"value\", \"ddate\"]]\n",
    "\n",
    "print(\"Cash Flow Sample:\\n\", cash_flow.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 'tlabel' column from Balance Sheet\n",
      "Dropped 'tlabel' column from Balance Sheet\n",
      "Dropped 'tlabel' column from Income Statement\n",
      "Dropped 'tlabel' column from Income Statement\n",
      "Dropped 'tlabel' column from Cash Flow\n",
      "Dropped 'tlabel' column from Cash Flow\n",
      "Denormalized fact tables saved successfully!\n"
     ]
    }
   ],
   "source": [
    "for df_name, df in [(\"Balance Sheet\", balance_sheet), \n",
    "                    (\"Income Statement\", income_statement), \n",
    "                    (\"Cash Flow\", cash_flow)]:\n",
    "    if \"tlabel\" in df.columns:\n",
    "        df.drop(columns=[\"tlabel\"], inplace=True)\n",
    "        print(f\"Dropped 'tlabel' column from {df_name}\")\n",
    "    if \"ticker\" in df.columns:\n",
    "        df.drop(columns=[\"ticker\"], inplace=True)\n",
    "        print(f\"Dropped 'tlabel' column from {df_name}\")\n",
    "\n",
    "balance_sheet.to_csv(\"balance_sheet.csv\", index=False)\n",
    "income_statement.to_csv(\"income_statement.csv\", index=False)\n",
    "cash_flow.to_csv(\"cash_flow.csv\", index=False)\n",
    "\n",
    "print(\"Denormalized fact tables saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted balance_sheet.csv -> balance_sheet.json\n",
      "Converted income_statement.csv -> income_statement.json\n",
      "Converted cash_flow.csv -> cash_flow.json\n",
      "All CSV files have been successfully converted to JSON!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file names\n",
    "csv_files = {\n",
    "    \"balance_sheet.csv\": \"balance_sheet.json\",\n",
    "    \"income_statement.csv\": \"income_statement.json\",\n",
    "    \"cash_flow.csv\": \"cash_flow.json\"\n",
    "}\n",
    "\n",
    "# Convert each CSV file to JSON\n",
    "for csv_file, json_file in csv_files.items():\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)  # Read CSV file\n",
    "        df.to_json(json_file, orient=\"records\", indent=4)  # Save as JSON\n",
    "        print(f\"Converted {csv_file} -> {json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {csv_file}: {e}\")\n",
    "\n",
    "print(\"All CSV files have been successfully converted to JSON!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large CSV file\n",
    "file_path = \"D:/NEU/SEM2/BigData/ASSIGNMENT2/cash_flow.csv\"\n",
    "df = pd.read_csv(file_path, chunksize=1000000)  # Adjust chunk size as needed\n",
    "\n",
    "# Split into multiple smaller CSV files\n",
    "for i, chunk in enumerate(df):\n",
    "    chunk.to_csv(f\"cash_flow_{i+1}.csv\", index=False)\n",
    "    print(f\"Created: cash_flow_part_{i+1}.csv\")\n",
    "\n",
    "print(\"CSV Splitting Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
